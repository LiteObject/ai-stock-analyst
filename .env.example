# AI Hedge Fund Environment Configuration
# =========================================

# Required API Keys
# -----------------
# Get your Financial Datasets API key from https://financialdatasets.ai/
FINANCIAL_DATASETS_API_KEY=your-financial-datasets-api-key

# Optional: Free Data Sources
# ---------------------------
# FRED API for economic indicators (free at https://fred.stlouisfed.org/docs/api/api_key.html)
# FRED_API_KEY=your-fred-api-key

# Web Search Configuration (for Hedge Fund Manager)
# -------------------------------------------------
# Tavily API for real-time news search (get API key at https://tavily.com/)
# TAVILY_API_KEY=your-tavily-api-key

# LLM Provider Configuration
# --------------------------
# Supported providers: openai, anthropic, ollama, google
LLM_PROVIDER=openai

# OpenAI (https://platform.openai.com/)
OPENAI_API_KEY=your-openai-api-key

# Anthropic (https://console.anthropic.com/) - optional
# ANTHROPIC_API_KEY=your-anthropic-api-key

# Google AI (https://ai.google.dev/) - optional
# GOOGLE_API_KEY=your-google-api-key

# Ollama (local, no API key required)
# OLLAMA_BASE_URL=http://localhost:11434

# LLM Model Configuration
# -----------------------
# Model to use (e.g., gpt-4o, claude-3-opus, gemini-pro, llama3:8b)
# DEFAULT_MODEL=gpt-4o

# Temperature for LLM responses (0.0-1.0, lower = more deterministic)
# MODEL_TEMPERATURE=0.0

# Caching Configuration
# ---------------------
# Enable/disable API response caching (true/false)
CACHE_ENABLED=true

# Cache time-to-live in hours
CACHE_TTL_HOURS=24

# Trading Configuration
# ---------------------
# Starting capital for trading simulation
# DEFAULT_INITIAL_CAPITAL=100000.0

# Maximum position size as percentage of portfolio
# MAX_POSITION_PERCENTAGE=0.20

# Maximum percentage of daily volume to trade
# LIQUIDITY_LIMIT_PERCENTAGE=0.10

# API Configuration
# -----------------
# Timeout for API requests in seconds
# API_TIMEOUT_SECONDS=30

# Maximum retries for failed API requests
# MAX_RETRIES=3

# Logging Configuration
# ---------------------
# Logging level (DEBUG, INFO, WARNING, ERROR, CRITICAL)
LOG_LEVEL=INFO

# Tracing Configuration (OpenTelemetry)
# -------------------------------------
# OpenTelemetry OTLP endpoint for tracing (default: AI Toolkit local endpoint)
# OTEL_EXPORTER_OTLP_ENDPOINT=http://localhost:4318
# To view traces, use VS Code AI Toolkit extension's trace viewer